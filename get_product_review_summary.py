# # -*- coding: utf-8 -*-
# """get_product_review_summary.ipynb

# Automatically generated by Colaboratory.

# Original file is located at
#     https://colab.research.google.com/drive/1PPvFLhFdxYwV_Xiy8tMlouTJhl5W4Swz
# """

# pip install vaderSentiment

import os
import json
import gzip
import pandas as pd
from urllib.request import urlopen
from pandas import DataFrame

import random
import numpy as np
from tqdm import tqdm_notebook as tqdm
from collections import defaultdict

import nltk
nltk.download('punkt')
from nltk.corpus import stopwords
nltk.download('stopwords')
from nltk.tokenize import word_tokenize, sent_tokenize
from nltk.stem import PorterStemmer
import re

from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer

def ProductReviewSummary(product_asin, sentiment_selection):
    # Load the data

    print ("point 1")

    data = []
    with gzip.open('Cell_Phones_and_Accessories_5.json.gz') as f:
        for l in f:
            data.append(json.loads(l.strip()))

    print ("point 2")
    # Load relevant columns from data
    textsumdf = DataFrame (data,columns=['asin','reviewText'])
    asin_review = DataFrame (textsumdf[textsumdf['asin'] == product_asin][['asin','reviewText']])

    # Apply VADER sentiment scores to all reviewText for the specific asi
    analyzer = SentimentIntensityAnalyzer()
    asin_review['compound'] = [analyzer.polarity_scores(x)['compound'] for x in asin_review['reviewText']]
    asin_review['neg'] = [analyzer.polarity_scores(x)['neg'] for x in asin_review['reviewText']]
    asin_review['neu'] = [analyzer.polarity_scores(x)['neu'] for x in asin_review['reviewText']]
    asin_review['pos'] = [analyzer.polarity_scores(x)['pos'] for x in asin_review['reviewText']]

    # User selects sentiment (good, bad, or all) they want to receive
    if sentiment_selection == "good":
      sub_asin_review = asin_review[asin_review['compound'] >= 0.5][['asin','reviewText']]
    elif sentiment_selection == "bad":
      sub_asin_review = asin_review[asin_review['compound'] < 0.5][['asin','reviewText']]
    elif sentiment_selection == "all":
      sub_asin_review = asin_review[['asin','reviewText']]
    else:
      print("Error in choice")

    sub_asin_review = DataFrame (sub_asin_review)

    # Group data by asin and concat all reviewText
    sub_asin_review['reviewText'] = sub_asin_review.groupby(['asin'])['reviewText'].transform(lambda x: '. '.join(x))
    sub_asin_review = sub_asin_review[['asin','reviewText']].drop_duplicates()

    text_string = sub_asin_review.reviewText.apply(str)

    # Pre-tokenize the sentences and generate concatted sentence without punctuation for frequency table
    tokenized_sentence = []
    for s in text_string:
      tokenized_sentence.append(sent_tokenize(s))

    tokenized_sentence = [y for x in tokenized_sentence for y in x] # flatten list

    freq_sentence = re.sub("[^a-zA-Z]", " ", str(tokenized_sentence))

    print("point 3")


    # Creation of frequency table
    def _create_frequency_table(text_string) -> dict:

        stopWords = set(stopwords.words("english"))
        words = word_tokenize(text_string)
        ps = PorterStemmer()

        freqTable = dict()
        for word in words:
            word = ps.stem(word)
            if word in stopWords:
                continue
            if word in freqTable:
                freqTable[word] += 1
            else:
                freqTable[word] = 1

        return freqTable

    # Scoring of sentences based on term frequencies
    def _score_sentences(sentences, freqTable) -> dict:
        sentenceValue = dict()

        for sentence in sentences:
            word_count_in_sentence = (len(word_tokenize(sentence)))
            for wordValue in freqTable:
                if wordValue in sentence.lower():
                    if sentence[:10] in sentenceValue:
                        sentenceValue[sentence[:10]] += freqTable[wordValue]
                    else:
                        sentenceValue[sentence[:10]] = freqTable[wordValue]

            sentenceValue[sentence[:10]] = sentenceValue[sentence[:10]] // word_count_in_sentence

        return sentenceValue

    # Calculating threshold for which sentences to keep
    def _find_average_score(sentenceValue) -> int:
        sumValues = 0
        for entry in sentenceValue:
            sumValues += sentenceValue[entry]

        # Average value of a sentence from original text
        average = int(sumValues / len(sentenceValue))

        return average

    # Generating the summary
    def _generate_summary(sentences, sentenceValue, threshold):
        sentence_count = 0
        summary = ''

        for sentence in sentences:
            if sentence[:10] in sentenceValue and sentenceValue[sentence[:10]] > (threshold):
                summary += " " + sentence
                sentence_count += 1

        return summary

    # Run code with all functions
    freq_table = _create_frequency_table(freq_sentence)
    sentences = tokenized_sentence
    sentence_scores = _score_sentences(sentences, freq_table)
    threshold = _find_average_score(sentence_scores)
    summary = _generate_summary(sentences, sentence_scores, 1.5 * threshold)

    return (summary)